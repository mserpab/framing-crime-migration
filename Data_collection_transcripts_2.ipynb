{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JcPTW02ROPT"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import deepl\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZKVP-NaROPW"
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGJPXElSROPX"
   },
   "outputs": [],
   "source": [
    "# API Key was removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RjG7yLcFROPY"
   },
   "outputs": [],
   "source": [
    "youtube = build(\"youtube\", \"v3\", developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhAA0o8YROPZ",
    "outputId": "f8985ebb-a52b-4f8a-fd2c-73864c30d056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@RPPNoticias - Channel ID: UC5j8-2FT0ZMMBkmK72R4aeA\n"
     ]
    }
   ],
   "source": [
    "CHANNEL_NAMES = [\"@RPPNoticias\"]\n",
    "\n",
    "\n",
    "# Defining function that fetches channel IDs\n",
    "def get_channel_ids(api_key, channel_names):\n",
    "\n",
    "    base_url = \"https://www.googleapis.com/youtube/v3/search\"\n",
    "    results = {}\n",
    "\n",
    "    for name in channel_names:\n",
    "        params = {\n",
    "            \"part\": \"snippet\",\n",
    "            \"q\": name,\n",
    "            \"type\": \"channel\",\n",
    "            \"key\": api_key\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=params).json()\n",
    "\n",
    "        if \"items\" in response and response[\"items\"]:\n",
    "            channel_id = response[\"items\"][0][\"id\"][\"channelId\"]\n",
    "            results[name] = channel_id\n",
    "        else:\n",
    "            results[name] = \"Not Found\"\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Fetch Channel IDs\n",
    "channel_ids = get_channel_ids(api_key, CHANNEL_NAMES)\n",
    "\n",
    "for name, channel_id in channel_ids.items():\n",
    "    print(f\"{name} - Channel ID: {channel_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xMlFSLKROPb"
   },
   "source": [
    "# Collecting videos for RPP Noticias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi9p9Y9PROPd",
    "outputId": "3cf692a0-bdd8-43d5-907a-4a09d1c8a66c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "start",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "end",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "25b6b351-8e07-4a6a-b2b4-967d62e0942b",
       "rows": [
        [
         "0",
         "2019-02-01T00:00:00-05:00",
         "2019-02-28T23:59:59-05:00"
        ],
        [
         "1",
         "2019-03-01T00:00:00-05:00",
         "2019-03-31T23:59:59-05:00"
        ],
        [
         "2",
         "2019-04-01T00:00:00-05:00",
         "2019-04-30T23:59:59-05:00"
        ],
        [
         "3",
         "2019-05-01T00:00:00-05:00",
         "2019-05-31T23:59:59-05:00"
        ],
        [
         "4",
         "2019-06-01T00:00:00-05:00",
         "2019-06-30T23:59:59-05:00"
        ],
        [
         "5",
         "2019-07-01T00:00:00-05:00",
         "2019-07-31T23:59:59-05:00"
        ],
        [
         "6",
         "2019-08-01T00:00:00-05:00",
         "2019-08-31T23:59:59-05:00"
        ],
        [
         "7",
         "2019-09-01T00:00:00-05:00",
         "2019-09-30T23:59:59-05:00"
        ],
        [
         "8",
         "2019-10-01T00:00:00-05:00",
         "2019-10-31T23:59:59-05:00"
        ],
        [
         "9",
         "2019-11-01T00:00:00-05:00",
         "2019-11-30T23:59:59-05:00"
        ],
        [
         "10",
         "2019-12-01T00:00:00-05:00",
         "2019-12-31T23:59:59-05:00"
        ],
        [
         "11",
         "2020-01-01T00:00:00-05:00",
         "2020-01-31T23:59:59-05:00"
        ],
        [
         "12",
         "2020-02-01T00:00:00-05:00",
         "2020-02-29T23:59:59-05:00"
        ],
        [
         "13",
         "2020-03-01T00:00:00-05:00",
         "2020-03-31T23:59:59-05:00"
        ],
        [
         "14",
         "2020-04-01T00:00:00-05:00",
         "2020-04-30T23:59:59-05:00"
        ],
        [
         "15",
         "2020-05-01T00:00:00-05:00",
         "2020-05-31T23:59:59-05:00"
        ],
        [
         "16",
         "2020-06-01T00:00:00-05:00",
         "2020-06-30T23:59:59-05:00"
        ],
        [
         "17",
         "2020-07-01T00:00:00-05:00",
         "2020-07-31T23:59:59-05:00"
        ],
        [
         "18",
         "2020-08-01T00:00:00-05:00",
         "2020-08-31T23:59:59-05:00"
        ],
        [
         "19",
         "2020-09-01T00:00:00-05:00",
         "2020-09-30T23:59:59-05:00"
        ],
        [
         "20",
         "2020-10-01T00:00:00-05:00",
         "2020-10-31T23:59:59-05:00"
        ],
        [
         "21",
         "2020-11-01T00:00:00-05:00",
         "2020-11-30T23:59:59-05:00"
        ],
        [
         "22",
         "2020-12-01T00:00:00-05:00",
         "2020-12-31T23:59:59-05:00"
        ],
        [
         "23",
         "2021-01-01T00:00:00-05:00",
         "2021-01-31T23:59:59-05:00"
        ],
        [
         "24",
         "2021-02-01T00:00:00-05:00",
         "2021-02-28T23:59:59-05:00"
        ],
        [
         "25",
         "2021-03-01T00:00:00-05:00",
         "2021-03-31T23:59:59-05:00"
        ],
        [
         "26",
         "2021-04-01T00:00:00-05:00",
         "2021-04-30T23:59:59-05:00"
        ],
        [
         "27",
         "2021-05-01T00:00:00-05:00",
         "2021-05-31T23:59:59-05:00"
        ],
        [
         "28",
         "2021-06-01T00:00:00-05:00",
         "2021-06-30T23:59:59-05:00"
        ],
        [
         "29",
         "2021-07-01T00:00:00-05:00",
         "2021-07-31T23:59:59-05:00"
        ],
        [
         "30",
         "2021-08-01T00:00:00-05:00",
         "2021-08-31T23:59:59-05:00"
        ],
        [
         "31",
         "2021-09-01T00:00:00-05:00",
         "2021-09-30T23:59:59-05:00"
        ],
        [
         "32",
         "2021-10-01T00:00:00-05:00",
         "2021-10-31T23:59:59-05:00"
        ],
        [
         "33",
         "2021-11-01T00:00:00-05:00",
         "2021-11-30T23:59:59-05:00"
        ],
        [
         "34",
         "2021-12-01T00:00:00-05:00",
         "2021-12-31T23:59:59-05:00"
        ],
        [
         "35",
         "2022-01-01T00:00:00-05:00",
         "2022-01-31T23:59:59-05:00"
        ],
        [
         "36",
         "2022-02-01T00:00:00-05:00",
         "2022-02-28T23:59:59-05:00"
        ],
        [
         "37",
         "2022-03-01T00:00:00-05:00",
         "2022-03-31T23:59:59-05:00"
        ],
        [
         "38",
         "2022-04-01T00:00:00-05:00",
         "2022-04-30T23:59:59-05:00"
        ],
        [
         "39",
         "2022-05-01T00:00:00-05:00",
         "2022-05-31T23:59:59-05:00"
        ],
        [
         "40",
         "2022-06-01T00:00:00-05:00",
         "2022-06-30T23:59:59-05:00"
        ],
        [
         "41",
         "2022-07-01T00:00:00-05:00",
         "2022-07-31T23:59:59-05:00"
        ],
        [
         "42",
         "2022-08-01T00:00:00-05:00",
         "2022-08-31T23:59:59-05:00"
        ],
        [
         "43",
         "2022-09-01T00:00:00-05:00",
         "2022-09-30T23:59:59-05:00"
        ],
        [
         "44",
         "2022-10-01T00:00:00-05:00",
         "2022-10-31T23:59:59-05:00"
        ],
        [
         "45",
         "2022-11-01T00:00:00-05:00",
         "2022-11-30T23:59:59-05:00"
        ],
        [
         "46",
         "2022-12-01T00:00:00-05:00",
         "2022-12-31T23:59:59-05:00"
        ],
        [
         "47",
         "2023-01-01T00:00:00-05:00",
         "2023-01-31T23:59:59-05:00"
        ],
        [
         "48",
         "2023-02-01T00:00:00-05:00",
         "2023-02-28T23:59:59-05:00"
        ],
        [
         "49",
         "2023-03-01T00:00:00-05:00",
         "2023-03-31T23:59:59-05:00"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 73
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-02-01T00:00:00-05:00</td>\n",
       "      <td>2019-02-28T23:59:59-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-01T00:00:00-05:00</td>\n",
       "      <td>2019-03-31T23:59:59-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-04-01T00:00:00-05:00</td>\n",
       "      <td>2019-04-30T23:59:59-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-01T00:00:00-05:00</td>\n",
       "      <td>2019-05-31T23:59:59-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-06-01T00:00:00-05:00</td>\n",
       "      <td>2019-06-30T23:59:59-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2024-10-01T00:00:00-05:00</td>\n",
       "      <td>2024-10-31T23:59:59-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2024-11-01T00:00:00-05:00</td>\n",
       "      <td>2024-11-30T23:59:59-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2024-12-01T00:00:00-05:00</td>\n",
       "      <td>2024-12-31T23:59:59-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2025-01-01T00:00:00-05:00</td>\n",
       "      <td>2025-01-31T23:59:59-05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2025-02-01T00:00:00-05:00</td>\n",
       "      <td>2025-02-28T23:59:59-05:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        start                        end\n",
       "0   2019-02-01T00:00:00-05:00  2019-02-28T23:59:59-05:00\n",
       "1   2019-03-01T00:00:00-05:00  2019-03-31T23:59:59-05:00\n",
       "2   2019-04-01T00:00:00-05:00  2019-04-30T23:59:59-05:00\n",
       "3   2019-05-01T00:00:00-05:00  2019-05-31T23:59:59-05:00\n",
       "4   2019-06-01T00:00:00-05:00  2019-06-30T23:59:59-05:00\n",
       "..                        ...                        ...\n",
       "68  2024-10-01T00:00:00-05:00  2024-10-31T23:59:59-05:00\n",
       "69  2024-11-01T00:00:00-05:00  2024-11-30T23:59:59-05:00\n",
       "70  2024-12-01T00:00:00-05:00  2024-12-31T23:59:59-05:00\n",
       "71  2025-01-01T00:00:00-05:00  2025-01-31T23:59:59-05:00\n",
       "72  2025-02-01T00:00:00-05:00  2025-02-28T23:59:59-05:00\n",
       "\n",
       "[73 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataframe with full month periods\n",
    "\n",
    "# Define the start and end dates\n",
    "start_date = datetime(2019, 2, 1)\n",
    "end_date = datetime(2025, 3, 1)\n",
    "\n",
    "# Create a list to hold the data\n",
    "data = []\n",
    "\n",
    "# Generate the start and end date-times for each full month period\n",
    "current_date = start_date\n",
    "while current_date < end_date:\n",
    "    # Get the start date of the current month\n",
    "    start_date_str = current_date.strftime(\"%Y-%m-%dT%H:%M:%S-05:00\")\n",
    "\n",
    "    # Calculate the next month's first day for the end date\n",
    "    next_month = current_date.replace(day=28) + timedelta(days=4)  # this gives us the next month\n",
    "    end_date_str = next_month.replace(day=1) - timedelta(seconds=1)\n",
    "    end_date_str = end_date_str.strftime(\"%Y-%m-%dT%H:%M:%S-05:00\")\n",
    "\n",
    "    # Append to the data list\n",
    "    data.append([start_date_str, end_date_str])\n",
    "\n",
    "    # Move to the first day of the next month\n",
    "    current_date = next_month.replace(day=1)\n",
    "\n",
    "\n",
    "date_ranges = pd.DataFrame(data, columns=[\"start\", \"end\"])\n",
    "\n",
    "date_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhsKCOfDROPf"
   },
   "outputs": [],
   "source": [
    "# new function\n",
    "#Function for getting video info for one channel id, one keyword, one time frame\n",
    "# Returns dictionary of all videos found and number of videos, and prints number\n",
    "#\n",
    "\n",
    "def get_youtube_videos(api_key, channel_id, keyword, max_results=50, order=\"date\",\n",
    "                       published_after=None, published_before=None):\n",
    "\n",
    "    videos = []\n",
    "\n",
    "    # Prepare API request parameters\n",
    "    params = {\n",
    "        \"part\": \"snippet\",\n",
    "        \"channelId\": channel_id,\n",
    "        \"q\": keyword,\n",
    "        \"type\": \"video\",\n",
    "        \"maxResults\": max_results,\n",
    "        \"order\": order,\n",
    "    }\n",
    "\n",
    "    if published_after:\n",
    "        params[\"publishedAfter\"] = published_after\n",
    "\n",
    "    if published_before:\n",
    "        params[\"publishedBefore\"] = published_before\n",
    "\n",
    "    # Execute API request\n",
    "    request = youtube.search().list(**params)\n",
    "    response = request.execute()\n",
    "\n",
    "    # Extract video details\n",
    "    for item in response.get(\"items\", []):\n",
    "        video_id = item[\"id\"][\"videoId\"]\n",
    "        video_title = item[\"snippet\"][\"title\"]\n",
    "        video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "        video_date = item[\"snippet\"][\"publishedAt\"]\n",
    "\n",
    "        videos.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"title\": video_title,\n",
    "            \"url\": video_url,\n",
    "            \"channel\": channel_id,\n",
    "            \"keyword\": keyword,\n",
    "            \"video_date\": video_date})\n",
    "\n",
    "\n",
    "    print(\"Number of videos found between \" + published_after + \" and \" + published_before + \": \" + str(len(videos)))\n",
    "\n",
    "\n",
    "\n",
    "    return videos, len(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuA4SZabROPg",
    "outputId": "74b6c5c4-2cf7-452f-aaee-ec0f1f23901a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos found between 2019-02-01T00:00:00-05:00 and 2019-02-28T23:59:59-05:00: 22\n",
      "Number of videos found between 2019-03-01T00:00:00-05:00 and 2019-03-31T23:59:59-05:00: 15\n",
      "Number of videos found between 2019-04-01T00:00:00-05:00 and 2019-04-30T23:59:59-05:00: 10\n",
      "Number of videos found between 2019-05-01T00:00:00-05:00 and 2019-05-31T23:59:59-05:00: 32\n",
      "Number of videos found between 2019-06-01T00:00:00-05:00 and 2019-06-30T23:59:59-05:00: 22\n",
      "Number of videos found between 2019-07-01T00:00:00-05:00 and 2019-07-31T23:59:59-05:00: 16\n",
      "Number of videos found between 2019-08-01T00:00:00-05:00 and 2019-08-31T23:59:59-05:00: 19\n",
      "Number of videos found between 2019-09-01T00:00:00-05:00 and 2019-09-30T23:59:59-05:00: 17\n",
      "Number of videos found between 2019-10-01T00:00:00-05:00 and 2019-10-31T23:59:59-05:00: 19\n",
      "Number of videos found between 2019-11-01T00:00:00-05:00 and 2019-11-30T23:59:59-05:00: 14\n",
      "Number of videos found between 2019-12-01T00:00:00-05:00 and 2019-12-31T23:59:59-05:00: 32\n",
      "Number of videos found between 2020-01-01T00:00:00-05:00 and 2020-01-31T23:59:59-05:00: 50\n",
      "Number of videos found between 2020-02-01T00:00:00-05:00 and 2020-02-29T23:59:59-05:00: 35\n",
      "Number of videos found between 2020-03-01T00:00:00-05:00 and 2020-03-31T23:59:59-05:00: 24\n",
      "Number of videos found between 2020-04-01T00:00:00-05:00 and 2020-04-30T23:59:59-05:00: 9\n",
      "Number of videos found between 2020-05-01T00:00:00-05:00 and 2020-05-31T23:59:59-05:00: 2\n",
      "Number of videos found between 2020-06-01T00:00:00-05:00 and 2020-06-30T23:59:59-05:00: 9\n",
      "Number of videos found between 2020-07-01T00:00:00-05:00 and 2020-07-31T23:59:59-05:00: 10\n",
      "Number of videos found between 2020-08-01T00:00:00-05:00 and 2020-08-31T23:59:59-05:00: 14\n",
      "Number of videos found between 2020-09-01T00:00:00-05:00 and 2020-09-30T23:59:59-05:00: 10\n",
      "Number of videos found between 2020-10-01T00:00:00-05:00 and 2020-10-31T23:59:59-05:00: 11\n",
      "Number of videos found between 2020-11-01T00:00:00-05:00 and 2020-11-30T23:59:59-05:00: 3\n",
      "Number of videos found between 2020-12-01T00:00:00-05:00 and 2020-12-31T23:59:59-05:00: 8\n",
      "Number of videos found between 2021-01-01T00:00:00-05:00 and 2021-01-31T23:59:59-05:00: 11\n",
      "Number of videos found between 2021-02-01T00:00:00-05:00 and 2021-02-28T23:59:59-05:00: 7\n",
      "Number of videos found between 2021-03-01T00:00:00-05:00 and 2021-03-31T23:59:59-05:00: 3\n",
      "Number of videos found between 2021-04-01T00:00:00-05:00 and 2021-04-30T23:59:59-05:00: 7\n",
      "Number of videos found between 2021-05-01T00:00:00-05:00 and 2021-05-31T23:59:59-05:00: 8\n",
      "Number of videos found between 2021-06-01T00:00:00-05:00 and 2021-06-30T23:59:59-05:00: 2\n",
      "Number of videos found between 2021-07-01T00:00:00-05:00 and 2021-07-31T23:59:59-05:00: 5\n",
      "Number of videos found between 2021-08-01T00:00:00-05:00 and 2021-08-31T23:59:59-05:00: 7\n",
      "Number of videos found between 2021-09-01T00:00:00-05:00 and 2021-09-30T23:59:59-05:00: 7\n",
      "Number of videos found between 2021-10-01T00:00:00-05:00 and 2021-10-31T23:59:59-05:00: 6\n",
      "Number of videos found between 2021-11-01T00:00:00-05:00 and 2021-11-30T23:59:59-05:00: 2\n",
      "Number of videos found between 2021-12-01T00:00:00-05:00 and 2021-12-31T23:59:59-05:00: 3\n",
      "Number of videos found between 2022-01-01T00:00:00-05:00 and 2022-01-31T23:59:59-05:00: 3\n",
      "Number of videos found between 2022-02-01T00:00:00-05:00 and 2022-02-28T23:59:59-05:00: 4\n",
      "Number of videos found between 2022-03-01T00:00:00-05:00 and 2022-03-31T23:59:59-05:00: 5\n",
      "Number of videos found between 2022-04-01T00:00:00-05:00 and 2022-04-30T23:59:59-05:00: 12\n",
      "Number of videos found between 2022-05-01T00:00:00-05:00 and 2022-05-31T23:59:59-05:00: 31\n",
      "Number of videos found between 2022-06-01T00:00:00-05:00 and 2022-06-30T23:59:59-05:00: 10\n",
      "Number of videos found between 2022-07-01T00:00:00-05:00 and 2022-07-31T23:59:59-05:00: 32\n",
      "Number of videos found between 2022-08-01T00:00:00-05:00 and 2022-08-31T23:59:59-05:00: 8\n",
      "Number of videos found between 2022-09-01T00:00:00-05:00 and 2022-09-30T23:59:59-05:00: 23\n",
      "Number of videos found between 2022-10-01T00:00:00-05:00 and 2022-10-31T23:59:59-05:00: 14\n",
      "Number of videos found between 2022-11-01T00:00:00-05:00 and 2022-11-30T23:59:59-05:00: 11\n",
      "Number of videos found between 2022-12-01T00:00:00-05:00 and 2022-12-31T23:59:59-05:00: 17\n",
      "Number of videos found between 2023-01-01T00:00:00-05:00 and 2023-01-31T23:59:59-05:00: 18\n",
      "Number of videos found between 2023-02-01T00:00:00-05:00 and 2023-02-28T23:59:59-05:00: 38\n",
      "Number of videos found between 2023-03-01T00:00:00-05:00 and 2023-03-31T23:59:59-05:00: 16\n",
      "Number of videos found between 2023-04-01T00:00:00-05:00 and 2023-04-30T23:59:59-05:00: 45\n",
      "Number of videos found between 2023-05-01T00:00:00-05:00 and 2023-05-31T23:59:59-05:00: 24\n",
      "Number of videos found between 2023-06-01T00:00:00-05:00 and 2023-06-30T23:59:59-05:00: 49\n",
      "Number of videos found between 2023-07-01T00:00:00-05:00 and 2023-07-31T23:59:59-05:00: 24\n",
      "Number of videos found between 2023-08-01T00:00:00-05:00 and 2023-08-31T23:59:59-05:00: 48\n",
      "Number of videos found between 2023-09-01T00:00:00-05:00 and 2023-09-30T23:59:59-05:00: 33\n",
      "Number of videos found between 2023-10-01T00:00:00-05:00 and 2023-10-31T23:59:59-05:00: 50\n",
      "Number of videos found between 2023-11-01T00:00:00-05:00 and 2023-11-30T23:59:59-05:00: 23\n",
      "Number of videos found between 2023-12-01T00:00:00-05:00 and 2023-12-31T23:59:59-05:00: 30\n",
      "Number of videos found between 2024-01-01T00:00:00-05:00 and 2024-01-31T23:59:59-05:00: 50\n",
      "Number of videos found between 2024-02-01T00:00:00-05:00 and 2024-02-29T23:59:59-05:00: 30\n",
      "Number of videos found between 2024-03-01T00:00:00-05:00 and 2024-03-31T23:59:59-05:00: 16\n",
      "Number of videos found between 2024-04-01T00:00:00-05:00 and 2024-04-30T23:59:59-05:00: 31\n",
      "Number of videos found between 2024-05-01T00:00:00-05:00 and 2024-05-31T23:59:59-05:00: 16\n",
      "Number of videos found between 2024-06-01T00:00:00-05:00 and 2024-06-30T23:59:59-05:00: 17\n",
      "Number of videos found between 2024-07-01T00:00:00-05:00 and 2024-07-31T23:59:59-05:00: 23\n",
      "Number of videos found between 2024-08-01T00:00:00-05:00 and 2024-08-31T23:59:59-05:00: 20\n",
      "Number of videos found between 2024-09-01T00:00:00-05:00 and 2024-09-30T23:59:59-05:00: 31\n",
      "Number of videos found between 2024-10-01T00:00:00-05:00 and 2024-10-31T23:59:59-05:00: 32\n",
      "Number of videos found between 2024-11-01T00:00:00-05:00 and 2024-11-30T23:59:59-05:00: 40\n",
      "Number of videos found between 2024-12-01T00:00:00-05:00 and 2024-12-31T23:59:59-05:00: 32\n",
      "Number of videos found between 2025-01-01T00:00:00-05:00 and 2025-01-31T23:59:59-05:00: 49\n",
      "Number of videos found between 2025-02-01T00:00:00-05:00 and 2025-02-28T23:59:59-05:00: 36\n"
     ]
    }
   ],
   "source": [
    "# applying new function\n",
    "# Create a for loop that goes through each time frame\n",
    "    # gets all youtube videos and store them in a dataframe\n",
    "\n",
    "channel_id = \"UC5j8-2FT0ZMMBkmK72R4aeA\" # RPP Noticias\n",
    "max_results = 50\n",
    "order = \"date\"\n",
    "keyword = \"robo|robó|roban|robaron|delincuentes|criminales|asalto|asaltó|asaltan|asaltaron|matan|mató|mataron|ladrón|asesinato|asesinado|asesinan|asesinaron|extorsionadores|extorsionador|extorsionan|extorsionaron|extorsionó|secuestro|secuestradores|secuestrada|secuestrado|secuestró|secuestraron|balacera|balean|balearon|balazos|sicariato|mafias|mafia|sicarios\"\n",
    "\n",
    "# Create empty dataframe for first time running it\n",
    "df = pd.DataFrame(columns=['video_id','title','url','channel','keyword','video_date'])\n",
    "videos_list = []\n",
    "videos_count_list = []\n",
    "\n",
    "for index, row in date_ranges.iterrows():\n",
    "    published_after = row['start']\n",
    "    published_before = row['end']\n",
    "\n",
    "\n",
    "    all = get_youtube_videos(api_key, channel_id, keyword, max_results,\n",
    "                                                          order, published_after, published_before)\n",
    "    videos = all[0]\n",
    "    video_count = all[1]\n",
    "    videos_list.append(videos)\n",
    "    videos_count_list.append(video_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_wEbsjjROPi"
   },
   "outputs": [],
   "source": [
    "flattened_list = np.concatenate(videos_list).tolist()\n",
    "df_R_1 = pd.DataFrame(flattened_list)\n",
    "df_R_1.drop(columns=['keyword'], inplace=True)\n",
    "date_ranges['videos_count'] = videos_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gf7wKDBPROPj",
    "outputId": "45eaad88-f160-4abc-d4d4-fd00635762ab"
   },
   "outputs": [],
   "source": [
    "df_R_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmpCXgi4ROPl",
    "outputId": "a6c2fa70-9e20-4e26-c72e-67a2f1f67f37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        start                        end\n",
      "0   2020-01-01T00:00:00-05:00  2020-01-07T23:59:59-05:00\n",
      "1   2020-01-08T00:00:00-05:00  2020-01-14T23:59:59-05:00\n",
      "2   2020-01-15T00:00:00-05:00  2020-01-21T23:59:59-05:00\n",
      "3   2020-01-22T00:00:00-05:00  2020-01-28T23:59:59-05:00\n",
      "4   2020-01-29T00:00:00-05:00  2020-01-31T23:59:58-05:00\n",
      "5   2023-10-01T00:00:00-05:00  2023-10-07T23:59:59-05:00\n",
      "6   2023-10-08T00:00:00-05:00  2023-10-14T23:59:59-05:00\n",
      "7   2023-10-15T00:00:00-05:00  2023-10-21T23:59:59-05:00\n",
      "8   2023-10-22T00:00:00-05:00  2023-10-28T23:59:59-05:00\n",
      "9   2023-10-29T00:00:00-05:00  2023-10-31T23:59:58-05:00\n",
      "10  2024-01-01T00:00:00-05:00  2024-01-07T23:59:59-05:00\n",
      "11  2024-01-08T00:00:00-05:00  2024-01-14T23:59:59-05:00\n",
      "12  2024-01-15T00:00:00-05:00  2024-01-21T23:59:59-05:00\n",
      "13  2024-01-22T00:00:00-05:00  2024-01-28T23:59:59-05:00\n",
      "14  2024-01-29T00:00:00-05:00  2024-01-31T23:59:58-05:00\n"
     ]
    }
   ],
   "source": [
    "#splitting to weekly periods\n",
    "split_periods = []\n",
    "\n",
    "df_filtered = date_ranges[date_ranges[\"videos_count\"] == 50].copy()\n",
    "\n",
    "\n",
    "# Convert to datetime format\n",
    "df_filtered[\"start\"] = pd.to_datetime(df_filtered[\"start\"])\n",
    "df_filtered[\"end\"] = pd.to_datetime(df_filtered[\"end\"])\n",
    "\n",
    "# Create a new list to store weekly periods\n",
    "weekly_periods = []\n",
    "\n",
    "# Loop through each full-month period and split into weeks\n",
    "for _, row in df_filtered.iterrows():\n",
    "    start_date = row[\"start\"]\n",
    "    end_date = row[\"end\"]\n",
    "\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date < end_date:\n",
    "        next_week = current_date + timedelta(days=7)\n",
    "        if next_week > end_date:\n",
    "            next_week = end_date  # Ensure the last week aligns with the month's end\n",
    "\n",
    "        weekly_periods.append({\n",
    "            \"start\": current_date,\n",
    "            \"end\": next_week - timedelta(seconds=1)  # End at 23:59:59 of the previous day\n",
    "        })\n",
    "\n",
    "        current_date = next_week  # Move to the next week\n",
    "\n",
    "# Create new DataFrame for weekly periods\n",
    "week_ranges = pd.DataFrame(weekly_periods)\n",
    "\n",
    "# Convert datetime format back to string\n",
    "week_ranges[\"start\"] = week_ranges[\"start\"].dt.strftime(\"%Y-%m-%dT%H:%M:%S-05:00\")\n",
    "week_ranges[\"end\"] = week_ranges[\"end\"].dt.strftime(\"%Y-%m-%dT%H:%M:%S-05:00\")\n",
    "\n",
    "# Display the result\n",
    "print(week_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7S_bYOT3ROPn",
    "outputId": "6f81e39f-4e3b-434e-c750-426bf54ff17d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos found between 2020-01-01T00:00:00-05:00 and 2020-01-07T23:59:59-05:00: 11\n",
      "Number of videos found between 2020-01-08T00:00:00-05:00 and 2020-01-14T23:59:59-05:00: 16\n",
      "Number of videos found between 2020-01-15T00:00:00-05:00 and 2020-01-21T23:59:59-05:00: 7\n",
      "Number of videos found between 2020-01-22T00:00:00-05:00 and 2020-01-28T23:59:59-05:00: 18\n",
      "Number of videos found between 2020-01-29T00:00:00-05:00 and 2020-01-31T23:59:58-05:00: 3\n",
      "Number of videos found between 2023-10-01T00:00:00-05:00 and 2023-10-07T23:59:59-05:00: 7\n",
      "Number of videos found between 2023-10-08T00:00:00-05:00 and 2023-10-14T23:59:59-05:00: 21\n",
      "Number of videos found between 2023-10-15T00:00:00-05:00 and 2023-10-21T23:59:59-05:00: 11\n",
      "Number of videos found between 2023-10-22T00:00:00-05:00 and 2023-10-28T23:59:59-05:00: 7\n",
      "Number of videos found between 2023-10-29T00:00:00-05:00 and 2023-10-31T23:59:58-05:00: 5\n",
      "Number of videos found between 2024-01-01T00:00:00-05:00 and 2024-01-07T23:59:59-05:00: 5\n",
      "Number of videos found between 2024-01-08T00:00:00-05:00 and 2024-01-14T23:59:59-05:00: 28\n",
      "Number of videos found between 2024-01-15T00:00:00-05:00 and 2024-01-21T23:59:59-05:00: 13\n",
      "Number of videos found between 2024-01-22T00:00:00-05:00 and 2024-01-28T23:59:59-05:00: 10\n",
      "Number of videos found between 2024-01-29T00:00:00-05:00 and 2024-01-31T23:59:58-05:00: 9\n"
     ]
    }
   ],
   "source": [
    "#Going through week_ranges\n",
    "# Create a for loop that goes through each time frame\n",
    "    # gets all youtube videos and store them in a dataframe\n",
    "\n",
    "channel_id = \"UC5j8-2FT0ZMMBkmK72R4aeA\" # RPP Noticias\n",
    "max_results = 50\n",
    "order = \"date\"\n",
    "keyword = \"robo|robó|roban|robaron|delincuentes|criminales|asalto|asaltó|asaltan|asaltaron|matan|mató|mataron|ladrón|asesinato|asesinado|asesinan|asesinaron|extorsionadores|extorsionador|extorsionan|extorsionaron|extorsionó|secuestro|secuestradores|secuestrada|secuestrado|secuestró|secuestraron|balacera|balean|balearon|balazos|sicariato|mafias|mafia|sicarios\"\n",
    "\n",
    "# Create empty dataframe for first time running it\n",
    "df = pd.DataFrame(columns=['video_id','title','url','channel','keyword','video_date'])\n",
    "videos_list = []\n",
    "videos_count_list = []\n",
    "\n",
    "for index, row in week_ranges.iterrows():\n",
    "    published_after = row['start']\n",
    "    published_before = row['end']\n",
    "\n",
    "\n",
    "    all = get_youtube_videos(api_key, channel_id, keyword, max_results,\n",
    "                                                          order, published_after, published_before)\n",
    "    videos = all[0]\n",
    "    video_count = all[1]\n",
    "    videos_list.append(videos)\n",
    "    videos_count_list.append(video_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4DEGZmJROPo"
   },
   "outputs": [],
   "source": [
    "flattened_list = np.concatenate(videos_list).tolist()\n",
    "df_R_2 = pd.DataFrame(flattened_list)\n",
    "df_R_2.drop(columns=['keyword'], inplace=True)\n",
    "\n",
    "week_ranges['videos_count'] = videos_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8zCx178ROPp"
   },
   "outputs": [],
   "source": [
    "df_merged = pd.concat([df_R_2, df_R_1], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgSx5fIvROPq",
    "outputId": "ffee15f7-7f6b-4e32-c6ff-836fd23d3150"
   },
   "outputs": [],
   "source": [
    "df_merged = df_merged.drop_duplicates(subset=\"video_id\", keep=\"first\")\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PIu65krROPq"
   },
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"videos_titles_RPPNoticias.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydICeQ-IROPr"
   },
   "source": [
    "# Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffrlMIPpROPr"
   },
   "outputs": [],
   "source": [
    "# trnascript function that stores also the error messages\n",
    "\n",
    "def get_transcript(video_id, language='es'):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=[language])\n",
    "        return \" \".join([line['text'] for line in transcript])\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "f5228eb63de0408696cd0f44c37fce74"
     ]
    },
    "id": "0kldPApBROPs",
    "outputId": "b3185400-ae9c-48ce-af70-0da35520ef7b"
   },
   "outputs": [],
   "source": [
    "# Attempt to retrieve transcripts for first 1000 failed cases\n",
    "\n",
    "for index, row in tqdm(df_merged.iterrows(), total=1500, desc=\"Retrying transcripts\"):\n",
    "    time.sleep(2)\n",
    "    df_merged.at[index, 'transcript'] = get_transcript(row['video_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAWV_b0LROPt",
    "outputId": "d80f833c-31af-4de2-9b09-1c9e68935529"
   },
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbRCyRhpROPu"
   },
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"transcripts_RPPNoticias.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9TiDBqYROPu"
   },
   "outputs": [],
   "source": [
    "df_merged = pd.read_csv(\"transcripts_RPPNoticias.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D0Jwer9ROPv",
    "outputId": "1915356e-e325-4adb-d373-21c3af83409e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transcripts with errors: 100\n"
     ]
    }
   ],
   "source": [
    "error_count = df_merged[df_merged['transcript'].str.startswith(\"Error\", na=False)].shape[0]\n",
    "print(f\"Total transcripts with errors: {error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nY3H3Rh4ROPw",
    "outputId": "8c68a0cf-4c2e-4903-d734-9fc85e1b2520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transcripts with age-restricted errors: 0\n",
      "Total transcripts with subtitles are disabled errors: 97\n",
      "Total transcripts with video is unplayable errors: 0\n",
      "Total transcripts with requested language codes: ['es'] errors: 3\n"
     ]
    }
   ],
   "source": [
    "error_count = df_merged[df_merged['transcript'].str.contains('age-restricted', case=False, na=False)].shape[0]\n",
    "print(f\"Total transcripts with age-restricted errors: {error_count}\")\n",
    "error_count = df_merged[df_merged['transcript'].str.contains('Subtitles are disabled', case=False, na=False)].shape[0]\n",
    "print(f\"Total transcripts with subtitles are disabled errors: {error_count}\")\n",
    "error_count = df_merged[df_merged['transcript'].str.contains('video is unplayable', case=False, na=False)].shape[0]\n",
    "print(f\"Total transcripts with video is unplayable errors: {error_count}\")\n",
    "error_count = df_merged[df_merged['transcript'].str.contains('requested language codes:', case=False, na=False)].shape[0]\n",
    "print(f\"Total transcripts with requested language codes: ['es'] errors: {error_count}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
